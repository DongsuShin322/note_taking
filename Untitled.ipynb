{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 해법영어교실"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome('/home/dongsu/Downloads/chromedriver')\n",
    "driver.get('http://www.hbenglish.co.kr/hbenglish/SearchClass.aspx?Code=010502')\n",
    "driver.maximize_window()\n",
    "\n",
    "df = pd.DataFrame(columns=['name','tel','addr'])\n",
    "\n",
    "text_area = driver.find_element_by_xpath(\"\"\"//*[@id=\"SearchWord\"]\"\"\")\n",
    "text_area.send_keys(\"서울\")\n",
    "driver.find_element_by_xpath(\"\"\"//*[@id=\"map_right2\"]/div/li/a\"\"\").click()\n",
    "time.sleep(2)\n",
    "text_area = driver.find_element_by_xpath(\"\"\"//*[@id=\"ctl00_ContentPlaceHolder1_SearchWord\"]\"\"\")\n",
    "text_area.clear()\n",
    "\n",
    "cnt = 1\n",
    "\n",
    "while True:\n",
    "    print ('page : {}'.format(cnt))\n",
    "    bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    time.sleep(0.2)\n",
    "    for b in bs.find_all('table', class_='map_addressRow'):\n",
    "        name = b.find_all('td')[0].text\n",
    "        tel = b.find_all('td')[1].text\n",
    "        addr = b.find_all('td')[2].text\n",
    "        df = df.append({'name':name, 'addr':addr, 'tel':tel}, ignore_index=True)\n",
    "    page_section = driver.find_element_by_id('ctl00_ContentPlaceHolder1_PageNumber')\n",
    "    try:\n",
    "        # next button 존재\n",
    "        page_section.find_elements_by_tag_name('span')[-2].find_element_by_tag_name('img')\n",
    "    except:\n",
    "        num = len(page_section.find_elements_by_tag_name('span'))\n",
    "        for i in range(num):\n",
    "            page = page_section.find_elements_by_tag_name('span')[i]\n",
    "            try:\n",
    "                # 현재 페이지\n",
    "                page.find_element_by_tag_name('b')\n",
    "            except:\n",
    "                continue\n",
    "            else:\n",
    "                page_idx = i\n",
    "                break\n",
    "        try:\n",
    "            # 다음 페이지 존재 확인(없으면 마지막 페이지)\n",
    "            page_section.find_elements_by_tag_name('span')[page_idx+1].click()\n",
    "            cnt += 1\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        page_section.find_elements_by_tag_name('span')[-2].click()\n",
    "        time.sleep(3)\n",
    "        cnt += 1\n",
    "        \n",
    "df.to_csv('/home/dongsu/Desktop/haebubEnglish.csv', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 온누리약국"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "base_url = 'http://www.onnuri.co.kr/store/store_01_sub.jsp?field=store_name&field_value=&store_name=&add1=&city_s_name=&city_l_name=&goo_name=&page='\n",
    "\n",
    "df = pd.DataFrame(columns=['name','addr','tel'])\n",
    "\n",
    "cnt = 1\n",
    "while True:\n",
    "    new_url = base_url + str(cnt)\n",
    "    req = requests.get(new_url)\n",
    "    html = req.content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    table = soup.find_all('table', attrs={'height':30})\n",
    "    if len(table) == 0:\n",
    "        break\n",
    "    else:\n",
    "        print ('page : {}'.format(cnt))\n",
    "        for s in table:\n",
    "            name = s.find_all('td')[1].text\n",
    "            name = name.replace('\\n','').replace('\\r','').replace('\\t','').strip()\n",
    "            addr = s.find_all('td')[3].text\n",
    "            tel = s.find_all('td')[4].text\n",
    "            tel = tel.replace('\\n','').strip()\n",
    "            df = df.append({'name':name, 'addr':addr, 'tel':tel}, ignore_index=True)\n",
    "        cnt += 1\n",
    "\n",
    "df.to_csv('/home/dongsu/Desktop/onnuri.csv', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 투다리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "df = pd.DataFrame(columns=['category','name','addr','tel'])\n",
    "\n",
    "url = \"http://www.tudari.co.kr/매장찾기/\"\n",
    "req = requests.get(url)\n",
    "html = req.content\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "last_page = int(soup.find('div', class_='pagination').find_all('a')[-3].text)\n",
    "\n",
    "for page in range(1, last_page+1):\n",
    "    url = \"http://www.tudari.co.kr/매장찾기/page/{}/\".format(page)\n",
    "    req = requests.get(url)\n",
    "    html = req.content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    time.sleep(0.2)\n",
    "    info = soup.find_all('tr')[1:-4]\n",
    "    print ('page : {}'.format(page))\n",
    "    for i in info:\n",
    "        category = i.find_all('td')[1].text\n",
    "        name = i.find_all('td')[2].find_all('a')[0].text\n",
    "        addr = i.find_all('td')[2].find_all('a')[-1].text.replace('\\xa0', ' ').replace('\\n','').strip()\n",
    "        tel = i.find('a',class_='mobile_tel')\n",
    "        if tel == None:\n",
    "            tel = ''\n",
    "        else:\n",
    "            tel = tel.text\n",
    "        df = df.append({'category':category,'name':name, 'addr':addr, 'tel':tel}, ignore_index=True)\n",
    "        \n",
    "df.to_csv('/home/dongsu/Desktop/tudari.csv', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 마스타"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 본죽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.select import Select\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('/home/dongsu/Downloads/chromedriver')\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bon_list = ['BF101','BF102','BF104','BF105']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in bon_list:\n",
    "    driver.get('https://www.bonif.co.kr/store/list?brdCd={}'.format(l))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.bonif.co.kr/store/list?brdCd={}'.format('BF101'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome('/home/dongsu/Downloads/chromedriver')\n",
    "driver.get('https://www.bbq.co.kr/shop/shopList.asp')\n",
    "driver.maximize_window()\n",
    "\n",
    "sido = ['서울','부산','대구','인천','광주','대전','울산','세종','경기','강원','충북','충남','전북','전남','경북','경남','제주']\n",
    "\n",
    "df = pd.DataFrame(columns=['name','addr','tel'])\n",
    "\n",
    "for i in range(len(sido)):\n",
    "    text_area = driver.find_element_by_xpath(\"\"\"//*[@id=\"search_text\"]\"\"\")\n",
    "    text_area.send_keys(sido[i])\n",
    "    time.sleep(3)\n",
    "    driver.find_element_by_xpath(\"\"\"//*[@id=\"search_form\"]/div/button\"\"\").click()\n",
    "    time.sleep(3)\n",
    "    bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    time.sleep(0.2)\n",
    "    text_area.clear()\n",
    "    stores = bs.find('div', class_='area on').find_all('a')\n",
    "    for store in stores:\n",
    "        name = store.find('p',class_='subject').text\n",
    "        addr = store.find('p',class_='add').text\n",
    "        tel = store.find('p',class_='tel').text\n",
    "        df = df.append({'name':name, 'addr':addr, 'tel':tel}, ignore_index=True)\n",
    "\n",
    "new_df = df.drop_duplicates()\n",
    "new_df.to_csv('/home/dongsu/Desktop/bbq.csv', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 빨간펜수학의달인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 삼성영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome('/home/dongsu/Downloads/chromedriver')\n",
    "driver.get('http://www.samsungenglish.com/customer/customer02.asp')\n",
    "driver.maximize_window()\n",
    "\n",
    "df = pd.DataFrame(columns=['name','addr','tel'])\n",
    "\n",
    "table = driver.find_element_by_xpath(\"\"\"//*[@id=\"right_contents\"]/table/tbody/tr[2]/td/div[3]/table/tbody/tr[2]/td/table\"\"\")\n",
    "num_area = len(table.find_elements_by_tag_name('a'))\n",
    "\n",
    "for i in range(num_area):\n",
    "    table = driver.find_element_by_xpath(\"\"\"//*[@id=\"right_contents\"]/table/tbody/tr[2]/td/div[3]/table/tbody/tr[2]/td/table\"\"\")\n",
    "    table.find_elements_by_tag_name('a')[i].click()\n",
    "    time.sleep(3)\n",
    "    pages = driver.find_element_by_xpath(\"\"\"//*[@id=\"result_txt\"]/div\"\"\")\n",
    "    num_pages = len(pages.find_elements_by_tag_name('a'))\n",
    "    for j in range(num_pages):\n",
    "        pages = driver.find_element_by_xpath(\"\"\"//*[@id=\"result_txt\"]/div\"\"\")\n",
    "        pages.find_elements_by_tag_name('a')[j].click()\n",
    "        time.sleep(3)\n",
    "        bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        time.sleep(0.2)\n",
    "        stores = bs.find('table', class_='bbs_property').find('tbody').find_all('tr')[1:]\n",
    "        for s in stores:\n",
    "            name = s.find_all('td')[0].text\n",
    "            tel = s.find_all('td')[1].text\n",
    "            addr = s.find_all('td')[2].text\n",
    "            df = df.append({'name':name, 'addr':addr, 'tel':tel}, ignore_index=True)\n",
    "            \n",
    "new_df = df.drop_duplicates()\n",
    "new_df.to_csv('/home/dongsu/Desktop/samsungenglish.csv', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 골프존파크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome('/home/dongsu/Downloads/chromedriver')\n",
    "driver.get('http://www.golfzonpark.com/StoreView/store_view.aspx')\n",
    "driver.maximize_window()\n",
    "\n",
    "df = pd.DataFrame(columns=['name','addr','tel'])\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    pages = driver.find_element_by_class_name('paging_area').find_elements_by_tag_name('a')\n",
    "    if pages[i].get_attribute('class') == 'paging_active active':\n",
    "        bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        time.sleep(0.2)\n",
    "        stores = bs.find('tbody', {'id':'tbodyList'}).find_all('tr')\n",
    "        for s in stores:\n",
    "            name = s.find('th').text\n",
    "            addr = s.find_all('td')[0].text.replace('\\xa0',' ').strip()\n",
    "            tel = s.find_all('td')[1].text\n",
    "            df = df.append({'name':name, 'addr':addr, 'tel':tel}, ignore_index=True)\n",
    "        if pages[i+1].get_attribute('class') == 'btn_paging_next disabled':\n",
    "            break\n",
    "        else:\n",
    "            pages[i+1].click()\n",
    "            time.sleep(1)\n",
    "            i = 0\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "df.to_csv('/home/dongsu/Desktop/golfzonpark.csv', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정관장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome('/home/dongsu/Downloads/chromedriver')\n",
    "driver.get('https://www.kgc.co.kr/front/store/korea/koreaList.do')\n",
    "\n",
    "df = pd.DataFrame(columns=['name','addr','tel'])\n",
    "\n",
    "sido = ['서울','부산','대구','인천','광주','대전','울산','세종','경기','강원','충북','충남','전북','전남','경북','경남','제주']\n",
    "\n",
    "for i in range(len(sido)):\n",
    "    text_area = driver.find_element_by_xpath(\"\"\"//*[@id=\"searchText\"]\"\"\")\n",
    "    text_area.send_keys(sido[i])\n",
    "    time.sleep(1)\n",
    "    text_area.send_keys(Keys.ENTER)\n",
    "    time.sleep(5)\n",
    "    bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    text_area.clear()\n",
    "    stores = bs.find('div', {'id':'Divresult'}).find_all('li')\n",
    "    for s in stores:\n",
    "        if not s.find('span', class_='icon_brand_k') == None:\n",
    "            name = s.find('p', class_='store_name').text.replace('\\t','').replace('\\n','').replace('\\xa0',' ')\n",
    "            addr = s.find('p', class_='store_address').contents[2]\n",
    "            tel = s.find('p', class_='store_address').contents[-1].replace('\\n','').replace('\\t','')\n",
    "            df = df.append({'name':name, 'addr':addr, 'tel':tel}, ignore_index=True)\n",
    "\n",
    "new_df = df.drop_duplicates()\n",
    "new_df.to_csv('/home/dongsu/Desktop/junggwanjang.csv', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 해법독서논술"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome('/home/dongsu/Downloads/chromedriver')\n",
    "driver.get('http://www.baccal.co.kr/About/ClassFind.aspx')\n",
    "\n",
    "df = pd.DataFrame(columns=['name', 'addr'])\n",
    "\n",
    "page = 1\n",
    "while True:\n",
    "    print (page)\n",
    "    bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    time.sleep(0.2)\n",
    "    stores = bs.find('table', class_='tableList').find('tbody').find_all('tr')\n",
    "    for s in stores:\n",
    "        name = s.find_all('td')[0].text\n",
    "        addr = s.find_all('td')[1].text\n",
    "        df = df.append({'name':name, 'addr':addr}, ignore_index=True)\n",
    "    try:\n",
    "        # next button 에 <a>태그 있으면 마지막 페이지 아님\n",
    "        driver.find_element_by_xpath(\"\"\"//*[@id=\"ContentPlaceHolder1_PageNumber\"]/span[3]/following-sibling::span\"\"\").find_element_by_tag_name('a').get_attribute('href')\n",
    "    except:\n",
    "        break\n",
    "    else:\n",
    "        driver.find_element_by_class_name('next').click()\n",
    "        time.sleep(3)\n",
    "        page += 1\n",
    "        \n",
    "df.to_csv('/home/dongsu/Desktop/haebubdokseo.csv', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 윤선생우리집앞영어교실 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.select import Select\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome('/home/dongsu/Downloads/chromedriver')\n",
    "driver.get('https://www.yoons.com/misc/center')\n",
    "\n",
    "df = pd.DataFrame(columns=['category', 'name', 'addr', 'tel'])\n",
    "\n",
    "select_sido = Select(driver.find_element_by_id('zone-a-0'))\n",
    "\n",
    "num_sido = len(select_sido.options)\n",
    "\n",
    "for i in range(num_sido):\n",
    "    select_sido = Select(driver.find_element_by_id('zone-a-0'))\n",
    "    select_sido.select_by_index(i)\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_css_selector(\"#searchButton\").click()\n",
    "    time.sleep(3)\n",
    "    bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    time.sleep(0.2)\n",
    "    stores = bs.find('div', class_='store-list').find_all('button', class_='store-info')\n",
    "    for s in stores:\n",
    "        category = s.contents[1].contents[2].replace('\\n','').rsplit(' ',maxsplit=1)[1].replace('(','').replace(')','')\n",
    "        name = s.contents[1].contents[2].replace('\\n','').rsplit(' ',maxsplit=1)[0]\n",
    "        addr = s.find('li', class_='address').contents[0].replace('\\n','')\n",
    "        if s.find('li', class_='tel') == None:\n",
    "            tel = ''\n",
    "        else:\n",
    "            tel = s.find('li', class_='tel').text\n",
    "        df = df.append({'category':category, 'name':name, 'addr':addr, 'tel':tel}, ignore_index=True)\n",
    "\n",
    "df.to_csv('/home/dongsu/Desktop/yoons.csv', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지앤비어학원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome('/home/dongsu/Downloads/chromedriver')\n",
    "driver.get('https://www.gnbenglish.com/map/map.php')\n",
    "\n",
    "df = pd.DataFrame(columns=['name', 'addr', 'tel'])\n",
    "\n",
    "sido_list = driver.find_element_by_id('sido_list').find_elements_by_tag_name('li')[:-1]\n",
    "for sido in sido_list:\n",
    "    sido.click()\n",
    "    time.sleep(1.5)\n",
    "    gungu_list = driver.find_element_by_id('gungu_list').find_elements_by_tag_name('li')\n",
    "    if len(gungu_list) != 1:\n",
    "        for gungu in gungu_list:\n",
    "            gungu.click()\n",
    "            time.sleep(1.5)\n",
    "            bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            time.sleep(0.2)\n",
    "            stores = bs.find('div', {'id':'camp_list'}).find_all('div', class_='mrsAddr')\n",
    "            for s in stores:\n",
    "                addr = s.find('p', class_='mrhWd01').text\n",
    "                name = s.find('p', class_='mrhWd02').find('span', class_='mrsadr01').text\n",
    "                tel = s.find('p', class_='mrhWd02').find('span', class_='mrsadr02').text.split(':')[1].strip()\n",
    "                df = df.append({'name':name, 'addr':addr, 'tel':tel}, ignore_index=True)\n",
    "\n",
    "new_df = df.drop_duplicates()\n",
    "new_df.to_csv('/home/dongsu/Desktop/gnb.csv', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 또래오래"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스피드메이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 능률 주니어랩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "req = requests.get('http://www.nelab.co.kr/pages/nelab/store.asp')\n",
    "html = req.content\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "last_page = int(soup.find('p', class_='paging').find_all('a')[-1].attrs['href'].split('currPageNo=')[1])\n",
    "\n",
    "df = pd.DataFrame(columns=['name', 'addr', 'tel'])\n",
    "\n",
    "base_url = 'http://www.nelab.co.kr/pages/nelab/store.asp?searchType=&searchText=&currPageNo={}'\n",
    "\n",
    "for i in range(1, last_page+1):\n",
    "    print ('page :', i)\n",
    "    req = requests.get(base_url.format(i))\n",
    "    html = req.content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    time.sleep(0.2)\n",
    "    stores = soup.find('table', class_='board_table').find('tbody').find_all('tr')\n",
    "    for s in stores:\n",
    "        name = s.find_all('td')[0].contents[0].text\n",
    "        try:\n",
    "            addr = s.find_all('td')[0].contents[2]\n",
    "        except:\n",
    "            addr = ''\n",
    "        tel = s.find_all('td')[1].text\n",
    "        df = df.append({'name':name, 'addr':addr, 'tel':tel}, ignore_index=True)\n",
    "        \n",
    "df.to_csv('/home/dongsu/Desktop/mnsoft/nelab.csv', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 옵티마"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로스펙스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.select import Select\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome('/home/dongsu/Downloads/chromedriver')\n",
    "driver.get('https://www.lsnmall.com/display.do?cmd=getStoreInfo&TCAT_CD=1501')\n",
    "\n",
    "df = pd.DataFrame(columns=['name', 'addr', 'tel', 'time'])\n",
    "\n",
    "select_sido = Select(driver.find_element_by_id('AREA_CD'))\n",
    "num = len(select_sido.options)\n",
    "for i in range(1, num):\n",
    "    select_sido.select_by_index(i)\n",
    "    time.sleep(0.5)\n",
    "    driver.find_element_by_css_selector('#container > div > div.store > div.faq_search > div > p.btn > button').click()\n",
    "    time.sleep(3)\n",
    "    bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    time.sleep(0.2)\n",
    "    stores = bs.find('div', class_='result_sc').find_all('div')[:-1]\n",
    "    for s in stores:\n",
    "        name = s.find('li', class_='name').text\n",
    "        addr = s.find('li', class_='addr').text\n",
    "        temp = s.find('li', class_='tel').text.replace('\\n','').replace('\\t','')\n",
    "        tel = temp.split('/')[0].replace('T.','').strip()\n",
    "        hour = temp.split('/')[1].split('(')[0].strip()\n",
    "        df = df.append({'name':name, 'addr':addr, 'tel':tel, 'time':hour}, ignore_index=True)\n",
    "        \n",
    "new_df = df.drop_duplicates()\n",
    "new_df.to_csv('/home/dongsu/Desktop/mnsoft/prospecs.csv', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 루마"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome('/home/dongsu/Downloads/chromedriver')\n",
    "driver.get('http://www.llumar.co.kr/sub_branch/automotive_list.php')\n",
    "\n",
    "df = pd.DataFrame(columns=['name', 'addr', 'tel'])\n",
    "\n",
    "while True:\n",
    "    if '다음페이지' not in driver.find_element_by_class_name('pageNum').find_elements_by_tag_name('li')[-1].text:\n",
    "        last_page = int(driver.find_element_by_class_name('pageNum').find_elements_by_tag_name('li')[-1].text)\n",
    "        break\n",
    "    else:\n",
    "        driver.find_element_by_class_name('pageNum').find_elements_by_tag_name('li')[-1].click()\n",
    "        time.sleep(3)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "base_url = 'http://www.llumar.co.kr/sub_branch/automotive_list.php?curr_page={}&list_count=20&sch_str=&area1=&area2=&sch_type='\n",
    "\n",
    "for i in range(1, last_page+1):\n",
    "    print ('page :', i)\n",
    "    req = requests.get(base_url.format(i))\n",
    "    html = req.content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    time.sleep(0.2)\n",
    "    stores = soup.find('div', class_='table-basic').find('tbody').find_all('tr')\n",
    "    for s in stores:\n",
    "        name = s.find_all('td')[1].text\n",
    "        addr = s.find_all('td')[2].text.strip()\n",
    "        tel = s.find_all('td')[4].text\n",
    "        df = df.append({'name':name, 'addr':addr, 'tel':tel}, ignore_index=True)\n",
    "        \n",
    "df.to_csv('/home/dongsu/Desktop/mnsoft/llumar.csv', encoding='cp949', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 휠라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:oracle]",
   "language": "python",
   "name": "conda-env-oracle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
